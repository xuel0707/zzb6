# global parameters
project_name: 'baike-process'
dataset_path: '/mnt/raw_data/baike'  # path to your dataset directory or file
export_path: '/mnt/outputs/process/baike/baike.jsonl'

export_shard_size: 0                                        # shard size of exported dataset in Byte. In default, it's 0, which means export the whole dataset into only one file. If it's set a positive number, the exported dataset will be split into several dataset shards, and the max size of each shard won't larger than the export_shard_size
export_in_parallel: false                                   # whether to export the result dataset in parallel to a single file, which usually takes less time. It only works when export_shard_size is 0, and its default number of processes is the same as the argument np. **Notice**: If it's True, sometimes exporting in parallel might require much more time due to the IO blocking, especially for very large datasets. When this happens, False is a better choice, although it takes more time.

np: 4  # number of subprocess to process your dataset
text_keys: 'title'    
text_keys: 'summary' 
text_keys: 'sections'    
text_keys: 'tags' 
text_keys: 'url' 
use_cache: true                                             # whether to use the cache management of Hugging Face datasets. It might take up lots of disk space when using cache
ds_cache_dir: '/mnt/datajucier_cache/huggingface/datasets'                                          # cache dir for Hugging Face datasets. In default, it\'s the same as the environment variable `HF_DATASETS_CACHE`, whose default value is usually "~/.cache/huggingface/datasets". If this argument is set to a valid path by users, it will override the default cache dir
open_monitor: true                                          # Whether to open the monitor to trace resource utilization for each OP during data processing. It\'s True in default.
use_checkpoint: false                                       # whether to use the checkpoint management to save the latest version of dataset to work dir when processing. Rerun the same config will reload the checkpoint and skip ops before it. Cache will be disabled when using checkpoint. If args of ops before the checkpoint are changed, all ops will be rerun from the beginning.
temp_dir: null                                              # the path to the temp directory to store intermediate caches when cache is disabled, these cache files will be removed on-the-fly. In default, it's None, so the temp dir will be specified by system. NOTICE: you should be caution when setting this argument because it might cause unexpected program behaviors when this path is set to an unsafe directory.
open_tracer: false                                            # whether to open the tracer to trace the changes during process. It might take more time when opening tracer
op_list_to_trace: []                                        # only ops in this list will be traced by tracer. If it's empty, all ops will be traced. Only available when tracer is opened.
trace_num: 10                                               # number of samples to show the differences between datasets before and after each op. Only available when tracer is opened.
op_fusion: false                                            # whether to fuse operators that share the same intermediate variables automatically. Op fusion might reduce the memory requirements slightly but speed up the whole process.
cache_compress: null                                        # the compression method of the cache file, which can be specified in ['gzip', 'zstd', 'lz4']. If this parameter is None, the cache file will not be compressed. We recommend you turn on this argument when your input dataset is larger than tens of GB and your disk space is not enough.

# for distributed processing
executor_type: default                                      # type of executor, support "default" or "ray" for now.
ray_address: auto                                           # the address of the Ray cluster.


# process schedule
# a list of several process operators with their arguments
process:
#   - chinese_convert_mapper:               # 繁简转换
#       mode: 't2s'


#   - special_characters_filter:           # 放宽特殊字符比例
#       min_ratio: 0.0
#       max_ratio: 0.5
#   - text_length_filter:                  # 降低文本长度限制
#       min_len: 0
#       max_len: 10000                                # whether to ignore sub-strings with specific pattern when computing simhash.
    - chinese_convert_mapper:               # 繁简转换
        mode: 't2s'
    - remove_non_chinese_character_mapper: # 移除非中文字符（保留字母、数字、标点）
        keep_alphabet: true
        keep_number: true
        keep_punc: true
    - remove_repeat_sentences_mapper:      # 去除重复句子
        min_repeat_sentence_length: 2
        ignore_special_character: true
    - remove_specific_chars_mapper:        # 清理特定符号
        chars_to_remove: '◆●■►▼▲▴∆▻▷❖♡□'
    - remove_table_text_mapper:            # 删除表格文本
        min_col: 2
        max_col: 20
    - clean_html_mapper: {}                # 去除HTML标签
    - clean_links_mapper: {}               # 去除链接
    - special_characters_filter:           # 放宽特殊字符比例
        min_ratio: 0.0
        max_ratio: 0.5
    - text_length_filter:                  # 降低文本长度限制
        min_len: 0
        max_len: 10000   